#+TITLE: Differentiable programming for scientists and engineers


* Broadcasting and Vectorized operations

pydoc:tensorflow.expand_dims

Tensorflow supports broadcasting practically the same way that numpy does. We can use pydoc:tensorflow.expand_dims to add dimensions to tensors, or we can use indexing with None to achieve the same thing. Here is an example.

#+BEGIN_SRC tf :results output drawer org
import tensorflow as tf

a = tf.constant([0, 1, 2])

b = tf.ones((3, 3), dtype=tf.int32)

c = b + a  # Adds a to every row of b

# These are two ways to add a columnwise to b
d = b + a[:, None]  #
e = b + tf.expand_dims(a, 1)

with tf.Session() as sess:
    print('row-wise:\n', sess.run(c))
    print('column-wise:')
    print(sess.run(d))
    print(sess.run(e))
#+END_SRC

#+RESULTS:
#+BEGIN_SRC org
row-wise:
 [[1 2 3]
 [1 2 3]
 [1 2 3]]
column-wise:
[[1 1 1]
 [2 2 2]
 [3 3 3]]
[[1 1 1]
 [2 2 2]
 [3 3 3]]
#+END_SRC

* Constructing tensors

pydoc:tensorflow.convert_to_tensor

pydoc:tensorflow.ones
pydoc:tensorflow.ones_like

pydoc:tensorflow.zeros
pydoc:tensorflow.zeros_like

pydoc:tensorflow.fill

pydoc:tensorflow.eye
pydoc:tensorflow.diag

pydoc:tensorflow.meshgrid

pydoc:tensorflow.concat

pydoc:tensorflow.stack

pydoc:tensorflow.reshape

pydoc:tensorflow.where

pydoc:tensorflow.split

** tf.tile
   
pydoc:tensorflow.tile

#+BEGIN_SRC tf :results output drawer org
import tensorflow as tf

n = tf.constant([1, 2, 3])

with tf.Session() as sess:
    print(sess.run(tf.tile(n[None, :], (5, 1))))
#+END_SRC

#+RESULTS:
#+BEGIN_SRC org
[[1 2 3]
 [1 2 3]
 [1 2 3]
 [1 2 3]
 [1 2 3]]
#+END_SRC


#+BEGIN_SRC tf :results output drawer org
import tensorflow as tf

inds = tf.constant([1, 2, 3])

n = tf.constant([1, 2, 3])

d = tf.tile(tf.ones_like(inds)[None, :], (tf.size(inds), 1))

ds = d * n[None, :]
with tf.Session() as sess:
    print(sess.run(ds))
#+END_SRC

#+RESULTS:
#+BEGIN_SRC org
[[1 2 3]
 [1 2 3]
 [1 2 3]]
#+END_SRC



** Make tensor from values and indices with a default value

We can use indexing to create arrays in numpy. Consider this example to make an identity matrix.

#+BEGIN_SRC python :results output org drawer
import numpy as np

a = np.zeros((3, 3))

a[[0, 1, 2], [0, 1, 2]] = [1, 2, 3]

print(a)
#+END_SRC

#+RESULTS:
:RESULTS:
[[ 1.  0.  0.]
 [ 0.  2.  0.]
 [ 0.  0.  3.]]
:END:

pydoc:tensorflow.sparse_to_dense is the way to create this as a tensor from the indices and values. You specify the indices where values are known, the shape of the /dense/ tensor that you want, and the values at the indices.

#+BEGIN_SRC tf :results output drawer org
import tensorflow as tf

a = tf.sparse_to_dense(sparse_indices=[[0, 0], [1, 1], [2, 2]],
                       output_shape=(3, 3),
                       sparse_values=[1, 2, 3])

with tf.Session() as sess:
    print(a.eval())
    print()
    # This is the simplest way to make this particular tensor
    print(tf.diag([1, 2, 3]).eval())
    print()
    # Here is another simple way
    print((tf.eye(3, 3, dtype=tf.int32) * tf.constant([1, 2, 3])).eval())
#+END_SRC

#+RESULTS:
#+BEGIN_SRC org
[[1 0 0]
 [0 2 0]
 [0 0 3]]

[[1 0 0]
 [0 2 0]
 [0 0 3]]

[[1 0 0]
 [0 2 0]
 [0 0 3]]
#+END_SRC


** logical

pydoc:tensorflow.less
pydoc:tensorflow.lessequal
pydoc:tensorflow.greater
pydoc:tensorflow.greaterequal
pydoc:tensorflow.equal

pydoc:tensorflow.logical_and
pydoc:tensorflow.logical_or
pydoc:tensorflow.logical_xor
pydoc:tensorflow.logical_not

* Accessing tensor components

pydoc:tensorflow.slice

pydoc:tensorflow.gather_nd

#+BEGIN_SRC tf :results output drawer org
import tensorflow as tf

z = tf.reshape(tf.range(9), (3, 3))

with tf.Session() as sess:
    print(z.eval())
    print('diag:  ', tf.diag_part(z).eval())
    print('row 1: ', z[1].eval())
    print('col 1: ', z[:, 1].eval())
    print('corners: ', tf.gather_nd(z, [[0, 0], [0, 2], [2, 0], [2, 2]]).eval())
#+END_SRC

#+RESULTS:
#+BEGIN_SRC org
[[0 1 2]
 [3 4 5]
 [6 7 8]]
diag:   [0 4 8]
row 1:  [3 4 5]
col 1:  [1 4 7]
corners:  [0 2 6 8]
#+END_SRC

* Assignments

To modify a tensor, it needs to be a pydoc:tensorflow.Variable.

** Change a variable

pydoc:tensorflow.assign is the main op to change the value of a variable.

#+BEGIN_SRC tf :results output drawer org
import tensorflow as tf

# An assignable variable
a = tf.Variable(3)

method1 = tf.assign(a, 5)

method2 = a.assign(4)

with tf.Session() as sess:
    # You have to initialize variables
    sess.run(tf.global_variables_initializer())

    method1.eval() # changes value of a
    print(a.eval())

    method2.eval() # changes value of a again
    print(a.eval())
#+END_SRC

#+RESULTS:
#+BEGIN_SRC org
5
4
#+END_SRC

Note there is also pydoc:tensorflow.assign_add and pydoc:tensorflow.assign_sub

#+BEGIN_SRC tf :results output drawer org
import tensorflow as tf

# An assignable variable
a = tf.Variable(3)

b = tf.assign_add(a, 1)

c = tf.assign_sub(a, 2)

with tf.Session() as sess:
    sess.run(tf.global_variables_initializer())

    print('add: ', b.eval())

    print('sub: ', c.eval())
#+END_SRC

#+RESULTS:
#+BEGIN_SRC org
add:  4
sub:  2
#+END_SRC

** Change part of a tensor

If your tensor is made of elements, you can use pydoc:tensorflow.scatter_update to change parts of it. It is a little subtle. You specify indices into the /first/ dimension of the variable. In 1-D this is easy to interpret, the indices are just the elements to replace. Here we set the second element of a tensor of ones to zero.

#+BEGIN_SRC tf :results output drawer org
import tensorflow as tf

a = tf.Variable(tf.ones((3,)))

u = tf.scatter_update(a, [1], [0])

with tf.Session() as sess:
    sess.run(tf.global_variables_initializer())
    print('Before: ', a.eval())
    sess.run(u)
    print('After:  ', a.eval())
#+END_SRC

#+RESULTS:
#+BEGIN_SRC org
Before:  [ 1.  1.  1.]
After:   [ 1.  0.  1.]
#+END_SRC

In higher dimensions, it is a little trickier. The indices to replace are for the first dimension, which means you have to replace /everything/ at those indices. So for a 2D tensor, the first index is for a row, so we have to replace the entire row with pydoc:tensorflow.scatter_update.

#+BEGIN_SRC tf :results output drawer org
import tensorflow as tf

a = tf.Variable(tf.ones((3, 3)))

u = tf.scatter_update(a, [1], [[0, 0, 0]])

with tf.Session() as sess:
    sess.run(tf.global_variables_initializer())
    print('Before:\n', a.eval())
    sess.run(u)
    print('After:\n', a.eval())
#+END_SRC

#+RESULTS:
#+BEGIN_SRC org
Before:
 [[ 1.  1.  1.]
 [ 1.  1.  1.]
 [ 1.  1.  1.]]
After:
 [[ 1.  1.  1.]
 [ 0.  0.  0.]
 [ 1.  1.  1.]]
#+END_SRC

** Sparse updates

pydoc:tensorflow.scatter_nd_update is used to make a /sparse/ update, i.e. just a few values in the tensor. You specify the indices to replace, and the replacement values.

#+BEGIN_SRC tf :results output drawer org
import tensorflow as tf

a = tf.Variable(tf.ones((3, 3)))

u = tf.scatter_nd_update(a, [[0, 1]], [2])

with tf.Session() as sess:    
    sess.run(tf.global_variables_initializer())
    print('Before:\n', a.eval())
    sess.run(u)
    print('After:\n', a.eval())
#+END_SRC

#+RESULTS:
#+BEGIN_SRC org
Before:
 [[ 1.  1.  1.]
 [ 1.  1.  1.]
 [ 1.  1.  1.]]
After:
 [[ 1.  2.  1.]
 [ 1.  1.  1.]
 [ 1.  1.  1.]]
#+END_SRC

* Iteration in tensorflow

Often we will iterate over some tensor and reduce it to some scalar value, e.g. a sum, average, product, or min/max. Tensorflow provides several "reduce_*" ops to achieve this.

pydoc:tensorflow.reduce_sum
pydoc:tensorflow.reduce_mean
pydoc:tensorflow.reduce_prod
pydoc:tensorflow.reduce_join

pydoc:tensorflow.reduce_max
pydoc:tensorflow.reduce_min

pydoc:tensorflow.reduce_all
pydoc:tensorflow.reduce_any


** Accumulating a sum
   :PROPERTIES:
   :ID:       29E4682F-DE66-4C84-904D-00334228B8C4
   :END:

Here are two ways in Python you might accumulate a sum using a for loop.

#+BEGIN_SRC python :results output org drawer
sum = 0

for i in range(5):
    sum += i

print(sum)

import numpy as np
print(np.sum(np.arange(5)))
#+END_SRC

#+RESULTS:
:RESULTS:
10
10
:END:

In TF, we use an approach more like numpy to accumulate the sum. pydoc:tensorflow.reduce_sum

#+BEGIN_SRC tf :results output drawer org
import tensorflow as tf

sum = tf.reduce_sum(tf.range(5))

with tf.Session() as sess:
    print(sum.eval())
#+END_SRC

#+RESULTS:
#+BEGIN_SRC org
10
#+END_SRC

For more complex operations that work iteratively there are pydoc:tensorflow.foldl and pydoc:tensorflow.foldr. These functions map a function onto the unpacked elements of a tensor. The function takes arguments of the last function value, and the current element. You can think of pydoc:tensorflow.foldl as working from the first element to the last, and pydoc:tensorflow.foldr working from the last to the first.

#+BEGIN_SRC tf :results output drawer org
import tensorflow as tf

sum = tf.foldl(lambda acc, curr: acc + curr, tf.range(5))

with tf.Session() as sess:
    print(sum.eval())
#+END_SRC

#+RESULTS:
#+BEGIN_SRC org
10
#+END_SRC

We can also use a loop-like approach in tensorflow. The syntax is quite different than Python though. You have to define two functions, one for the conditional part of the loop, and one for the body of the loop. Both functions take all the loop variables. Here we have two loop variables we need to track: a counter, and the accumulated sum. We want the loop to proceed as long as the counter is less than 5. We create this in a lambda function since it is just one line. In the body we have to accumulate the sum and increment the counter. The body function should return both of these values.

Finally, we pass initial values of the loop_vars to the while_loop. The while_loop returns the loop variables, which we can evaluate in a tf.Session.

#+BEGIN_SRC tf :results output drawer org
import tensorflow as tf

i = tf.constant(0, dtype=tf.int32)
sum = tf.constant(0, dtype=tf.int32)

cond = lambda i, sum: i < 5

def body(i, sum):
    sum = tf.add(sum, i)
    i = tf.add(i, 1)
    return i, sum

_i, _sum = tf.while_loop(cond, body, loop_vars=[i, sum])

with tf.Session() as sess:
    print(_sum.eval())
#+END_SRC

#+RESULTS:
#+BEGIN_SRC org
10
#+END_SRC


** Mimicing list comprehension

Consider this list comprehension in Python. It is easy to accomplish the same thing with element-wise operations in numpy.

#+BEGIN_SRC python :results output org drawer
print([i**2 for i in range(5)])

import numpy as np
a = np.arange(5)
print(a**2)
#+END_SRC

#+RESULTS:
:RESULTS:
[0, 1, 4, 9, 16]
[ 0  1  4  9 16]
:END:

We can also use element-wise operations in tensorflow.

#+BEGIN_SRC tf :results output drawer org
import tensorflow as tf
a = tf.range(5)

with tf.Session() as sess:
    print(sess.run(a**2))
#+END_SRC

#+RESULTS:
#+BEGIN_SRC org
[ 0  1  4  9 16]
#+END_SRC

It is possible to do something more like list comprehension, which can be helpful for more complex operations. To do this, use pydoc:tensorflow.unstack to unpack a tensor into a list of tensors, do the list comprehension on this list, and then use pydoc:tensorflow.stack to put them back together.

#+BEGIN_SRC tf :results output drawer org
import tensorflow as tf

a = tf.range(5)

lc = tf.stack([x**2 for x in tf.unstack(a)])

with tf.Session() as sess:
    print(lc.eval())
#+END_SRC

#+RESULTS:
#+BEGIN_SRC org
[ 0  1  4  9 16]
#+END_SRC

pydoc:tensorflow.scan

To get this in TF, we can use the tf.scan op. This op takes a function as its first argument, and a sequence as its second argument. The result is obtained by applying the function to each element of the sequence. The function in the first argument should take two arguments, the value from the previous function call, and the current element.

#+BEGIN_SRC tf :results output drawer org
import tensorflow as tf

res = tf.scan(lambda acc, curr: curr**2, tf.range(5))

with tf.Session() as sess:
    print(res.eval())
#+END_SRC

#+RESULTS:
#+BEGIN_SRC org
[ 0  1  4  9 16]
#+END_SRC

Another approach is to use the tf.map_fn op.

pydoc:tensorflow.map_fn

#+BEGIN_SRC tf :results output drawer org
import tensorflow as tf
import numpy as np
X = tf.range(5)

def f(x):
    return x**2

res = tf.map_fn(f, X)

with tf.Session() as sess:
    print(sess.run(res))
#+END_SRC

#+RESULTS:
#+BEGIN_SRC org
[ 0  1  4  9 16]
#+END_SRC



You can use the value from the previous call to accumulate something, e.g. a cumulative sum. This is not something that can be done with tf.reduce_sum, for example.

#+BEGIN_SRC python :results output org drawer
import numpy as np
print(np.cumsum(np.arange(5)))
#+END_SRC

#+RESULTS:
:RESULTS:
[ 0  1  3  6 10]
:END:

Tensorflow does have pydoc:tensorflow.cumsum:

#+BEGIN_SRC tf :results output drawer org
import tensorflow as tf

res = tf.cumsum(tf.range(5))

with tf.Session() as sess:
    print(res.eval())
#+END_SRC

#+RESULTS:
#+BEGIN_SRC org
[ 0  1  3  6 10]
#+END_SRC

But we can create our one version in one line like this.

#+BEGIN_SRC tf :results output drawer org
import tensorflow as tf

res = tf.scan(lambda acc, curr: acc + curr, tf.range(5))

with tf.Session() as sess:
    print(res.eval())
#+END_SRC

#+RESULTS:
#+BEGIN_SRC org
[ 0  1  3  6 10]
#+END_SRC

** Something like enumerate

#+BEGIN_SRC tf :results output drawer org
import tensorflow as tf

a = tf.range(5)

lc = tf.stack([(i, x**2) for i, x in enumerate(tf.unstack(a))])

with tf.Session() as sess:
    print(lc.eval())
#+END_SRC

#+RESULTS:
#+BEGIN_SRC org
[[ 0  0]
 [ 1  1]
 [ 2  4]
 [ 3  9]
 [ 4 16]]
#+END_SRC

** tf.while_loop

pydoc:tensorflow.while_loop

Here is how we use the while_loop to accumulate the sum. It is better to use tf.reduce_sum on this.

#+BEGIN_SRC tf :results output drawer org
import tensorflow as tf

i = tf.constant(0)
sum = tf.constant(0)

cond = lambda i, sum: i < 5

def body(i, sum):
    sum = sum + i
    i = i + 1
    return i, sum

i, sum = tf.while_loop(cond, body, [i, sum])
with tf.Session() as sess:
    print(sess.run(sum))
#+END_SRC

#+RESULTS:
#+BEGIN_SRC org
10
#+END_SRC


I find the tensorflow version a little unintuitive because you have to pass loop vars around. This approach creates a variable that you retrieve in the body, but it is not much easier to use in my opinion.

#+BEGIN_SRC tf :results output drawer org
import tensorflow as tf

i = tf.constant(0, dtype=tf.int32)

with tf.variable_scope("foo", reuse=tf.AUTO_REUSE):
    sum = tf.get_variable("sum", (), dtype=tf.int32)
    sum = tf.assign(sum, 0)

cond = lambda i: i < 5

def body(i):
    with tf.variable_scope("foo", reuse=tf.AUTO_REUSE):
        sum = tf.get_variable("sum", (), dtype=tf.int32)
    sum = tf.assign(sum, sum + i)
    with tf.control_dependencies([sum]):
        return tf.add(i, 1)

i = tf.while_loop(cond, body, [i])

with tf.Session() as sess:
    sess.run(tf.global_variables_initializer())
    print(sess.run([i, sum]))
#+END_SRC

#+RESULTS:
#+BEGIN_SRC org
[5, 10]
#+END_SRC

*** Using namedtuples in loops

Consider this while_loop example (derived from https://www.tensorflow.org/api_docs/python/tf/while_loop). We want to compute two values, j, and k, where j starts at 1, and k starts at 2. For each step, we compute j = j + k and k = j - k, for 10 steps.

It works, but we have to make up names for the loop variables inside the loop.

#+BEGIN_SRC tf :results output drawer org
import tensorflow as tf

lvars0 = (tf.constant(0), tf.constant(1), tf.constant(2))

def cond(*vars):
    return vars[0] < 10

def body(*vars):
    i, j, k = vars
    return (i + 1, j + k, j - k)

lvars1 = tf.while_loop(cond, body, lvars0)

with tf.Session() as sess:
    print(sess.run(lvars1))
#+END_SRC

#+RESULTS:
#+BEGIN_SRC org
(10, 32, 64)
#+END_SRC

A way around this is the use of a namedtuple. There are some subtleties, like you have to set loop_vars to a list or tuple. But, you can access information by named fields with this.

#+BEGIN_SRC tf :results output drawer org
import tensorflow as tf

import collections

Pair = collections.namedtuple('Pair', 'i, j, k')

# initialize the input,  Must be a tuple or list.
ijk_0 = (Pair(tf.constant(0), tf.constant(1), tf.constant(2)),)

def cond(p):
 return p.i < 10  # access the counter by name

def body(p):
    p = Pair(p.i + 1, (p.j + p.k), (p.j - p.k))
    return p, # You have to return a tuple

ijk_final = tf.while_loop(cond, body, ijk_0)

with tf.Session() as sess:
    print(sess.run(ijk_final))
#+END_SRC

#+RESULTS:
#+BEGIN_SRC org
(Pair(i=10, j=32, k=64),)
#+END_SRC


** Accumulating a list

Say we want to accumulate a list of even numbers. In Python, we can do it like this.

#+BEGIN_SRC python :results output org drawer
res = []
for i in range(5):
    if i % 2 == 0:
        res += [i]
print(res)
# or like this
print([i for i in range(5) if i % 2 == 0])

# numpy approach
import numpy as np
a = np.arange(5)
print(a[a % 2 == 0])
#+END_SRC

#+RESULTS:
:RESULTS:
[0, 2, 4]
[0, 2, 4]
[0 2 4]
:END:


pydoc:tensorflow.boolean_mask

The best approach is probably the numpy like one using a mask like this. Basically we use it as a filter to select just what we want from a larger list.

#+BEGIN_SRC tf :results output drawer org
import tensorflow as tf

a = tf.range(5)
even = tf.equal(tf.mod(a, 2), 0)
res = tf.boolean_mask(a, even)

with tf.Session() as sess:
    print(sess.run(res))
#+END_SRC

#+RESULTS:
#+BEGIN_SRC org
[0 2 4]
#+END_SRC

We can also  use a while_loop op.

pydoc:tensorflow.while_loop

We use the tf.while_loop to achieve this. Since we have a list we are accumulating that will change size, we need to use the shape_invariants option in the tf.while_loop. You have to specify some information about each loop variable. The counter i will not change shape, so we just get its shape. The list will change, but we have only one dimension, so we specify the shape in that dimension to be None, indicating we are not saying how big it will get.

The body of this loop took several unsuccessful iterations of more obvious approaches that didn't work. For example, you cannot use standard "if" conditional statements. The tf.cond op here does what we want, calling a true_fn on the condition that i is even, and the false_fn otherwise.

#+BEGIN_SRC tf :results output drawer org
import tensorflow as tf

i = tf.constant(0, dtype=tf.int32)
res = tf.Variable([], dtype=tf.int32)

# run the loop as long as i is less than 5.
def cond(i, res):
    return i < 5

# if i is even, add it to the res list, and increment i in the body.
def body(i, res):
    res = tf.cond(tf.equal(i % 2, 0),
                  true_fn=lambda: tf.concat([res, [i]], axis=0),
                  false_fn=lambda: res)

    i = tf.add(i, 1)
    return i, res

counter, loop = tf.while_loop(cond, body, loop_vars=[i, res],
                       shape_invariants=[i.get_shape(), tf.TensorShape(None)])

with tf.Session() as sess:
    print(loop.eval())
#+END_SRC

#+RESULTS:
#+BEGIN_SRC org
[0 2 4]
#+END_SRC

** Nested iteration

Consider this nested list prototype where the inner list depends on the value of outer counter.

#+BEGIN_SRC python :results output org drawer
res = []
for i in range(3):
    for j in range(i + 1, 3):
        res += [(i, j)]

print(res)
#+END_SRC

#+RESULTS:
:RESULTS:
[(0, 1), (0, 2), (1, 2)]
:END:

This can be achieved without any loops. We can generate these indices from tensor operations. The idea is to generate all the i, j combinations with a meshgrid, and then find the indices where the condition that j >= i + 1 is true, then use those indices to select the pairs of i and j that apply.

#+BEGIN_SRC tf :results output drawer org
import tensorflow as tf
import tensorflow.contrib.eager as tfe

tfe.enable_eager_execution()

i, j = tf.meshgrid(tf.range(3), tf.range(3))
inds = j >= i + 1

res = tf.stack([tf.boolean_mask(i, inds),
                tf.boolean_mask(j, inds)], axis=1)

print(res)
#+END_SRC

#+RESULTS:
#+BEGIN_SRC org
tf.Tensor(
[[0 1]
 [0 2]
 [1 2]], shape=(3, 2), dtype=int32)
#+END_SRC


It is possible to use nested loops in tensorflow. We have to use shape_invariants since our list is changing size. I was not able to figure out how to get an empty tensor with the required shape, so we initialize a constant with a throw away value, and use indexing at the end.

#+BEGIN_SRC tf :results output drawer org
import tensorflow as tf

i = tf.constant(0, dtype=tf.int32)

# Note we initialize with a fake value. I don't know how to specify the shape
# otherwise since it is not 1d.
res = tf.constant([[-1, -1]], dtype=tf.int32)

outer_cond = lambda i, res: i < 3

def outer_body(i, res):
    j = tf.add(i, 1)

    inner_cond = lambda j, res: j < 3
    def inner_body(j, res):
        res = tf.concat([res, [[i, j]]], axis=0)
        j += 1

        return j, res

    j, res = tf.while_loop(inner_cond, inner_body, [j, res],
                           shape_invariants=[j.get_shape(),
                                             tf.TensorShape([None, 2])])
    i += 1
    return i, res

i, res = tf.while_loop(outer_cond, outer_body, [i, res],
                       shape_invariants=[i.get_shape(),
                                         tf.TensorShape([None, 2])])

# Throw away the first row.
res = res[1:, :]

with tf.Session() as sess:
    print(sess.run(res))
#+END_SRC

#+RESULTS:
#+BEGIN_SRC org
[[0 1]
 [0 2]
 [1 2]]
#+END_SRC


For fun, we look at an example with a namedtuple, where we will accumulate both even and odd numbers. Here we map over a range of numbers, collecting the odd numbers in one place, and the even numbers in another. This was subtle. We have arrays changing shape inside a named tuple, so we have to use shape_invariants. What was not obvious (or possibly even documented?) is you have to put the shape invariants inside another namedtuple.

#+BEGIN_SRC tf :results output drawer org
import collections
import tensorflow as tf

LV = collections.namedtuple('LoopVariables', 'i, odd, even')

lv0 = LV(tf.constant(0, dtype=tf.int32),
         tf.Variable([], dtype=tf.int32),
         tf.Variable([], dtype=tf.int32))

def cond(nt):
    return nt.i < 11

def body(nt):

    def truef():
        return LV(nt.i + 1, nt.odd, tf.concat([nt.even, [nt.i]], axis=0))

    def falsef():
        return LV(nt.i + 1, tf.concat([nt.odd, [nt.i]], axis=0), nt.even)

    res = tf.cond(tf.equal(tf.mod(nt.i, 2), 0),
                  true_fn=truef,
                  false_fn=falsef)
    return res,


lv1, = tf.while_loop(cond, body, [lv0], shape_invariants=[LV(
    lv0.i.get_shape(), tf.TensorShape(None), tf.TensorShape(None))])


with tf.Session() as sess:
    print(sess.run(lv1))
#+END_SRC

#+RESULTS:
#+BEGIN_SRC org
LoopVariables(i=11, odd=array([1, 3, 5, 7, 9], dtype=int32), even=array([ 0,  2,  4,  6,  8, 10], dtype=int32))
#+END_SRC

Finally, what if want to collect numbers into a dynamically determined number of locations? Here we try to split a tensor into N tensors based on the value of each element mod N. We want this to be dynamic, e.g. that we can change N. I don't know a way to access loopvars by either an index, or a named field in Tensorflow, so we have a single list here that we store the index and value, and then after the loop construct the tensors we want. This idea came from Steve Kearnes. Thanks Steve!

#+BEGIN_SRC tf :results output drawer org
import collections
import tensorflow as tf

N = 4

LV = collections.namedtuple('LoopVariables', 'i, tuples')

loopvars = [tf.constant(0, dtype=tf.int32), 
            tf.Variable(tf.zeros((0, 2), dtype=tf.int32), dtype=tf.int32)]

lv0 = LV(*loopvars)

shiv = LV(loopvars[0].get_shape(), tf.TensorShape([None, 2]))

def cond(nt):
    return nt.i < 21

def body(nt):
    i = nt.i   
    n = tf.mod(nt.i, N)
    v = tf.concat([nt.tuples, [[n, i]]], axis=0)
    
    return LV(i + 1, v),

res, = tf.while_loop(cond, body, [lv0], [shiv])

arrs = [tf.gather(res.tuples, tf.where(tf.equal(res.tuples[:, 0], i)))[:, :, 1]
        for i in range(N)]

with tf.Session() as sess:
    sess.run(tf.global_variables_initializer())
    _arrs = sess.run(arrs)
    for i in range(N):
        print(i, _arrs[i].flatten())
#+END_SRC

#+RESULTS:
#+BEGIN_SRC org
0 [ 0  4  8 12 16 20]
1 [ 1  5  9 13 17]
2 [ 2  6 10 14 18]
3 [ 3  7 11 15 19]
#+END_SRC



* Data structures in tensorflow

** TODO dictionary / hashtable

pydoc:tensorflow.contrib.lookup.HashTable
pydoc:tensorflow.contrib.lookup.KeyValueTensorInitializer

HashTables are basically dictionaries with default values in Tensorflow. The are not as easy to construct though, and they are immutable after you initialize them.

1. Identify keys and values. These will be passed to a KeyValueTensorInitializer.
2. Create a KeyValueTensorInitializer that takes the keys and values, and optionally specifies their dtypes.
3. Create the table with the initializer with a mandatory default value.
4. In the session, initialize the table.
5. then you can lookup values by tensors.

#+BEGIN_SRC tf :results output drawer org
import tensorflow as tf

# here are some keys and 
keys = ['a', 'b']
values = [1, 2]

ht_init = tf.contrib.lookup.KeyValueTensorInitializer(keys, values)
table = tf.contrib.lookup.HashTable(ht_init, default_value=-1)

input_tensor = tf.constant('a')
out = table.lookup(input_tensor)

with tf.Session():
    table.init.run()  # Table initialization step
    print(out.eval()) # The actual lookup
#+END_SRC

#+RESULTS:
#+BEGIN_SRC org
1
#+END_SRC


* Sorting and Unique elements in tensorflow

Get the unique values. They are unsorted.

#+BEGIN_SRC tf :results output drawer org
import tensorflow as tf

s = tf.unique([1, 4, 1, 0, 0])

with tf.Session() as sess:
    print(sess.run(s.y))
#+END_SRC

#+RESULTS:
#+BEGIN_SRC org
[1 4 0]
#+END_SRC


** Sorting

#+BEGIN_SRC tf :results output drawer org
import tensorflow as tf

a = tf.constant([3, 1, 2, 1])

srt, srtargs = tf.nn.top_k(a, k=tf.reduce_prod(a.get_shape()))

with tf.Session() as sess:
    print(sess.run(srt))
    print(sess.run(srtargs))
#+END_SRC

#+RESULTS:
#+BEGIN_SRC org
[3 2 1 1]
[0 2 1 3]
#+END_SRC

To get the tensor sorted in ascending order, multiply the tensor by -1, and then multiply the sorted tensor by -1 again.

#+BEGIN_SRC tf :results output drawer org
import tensorflow as tf

a = tf.constant([3, 1, 2, 1])

srt, srtargs = tf.nn.top_k(-a, k=tf.reduce_prod(a.get_shape()))

with tf.Session() as sess:
    print(sess.run(-srt))
    print(sess.run(srtargs))
#+END_SRC

#+RESULTS:
#+BEGIN_SRC org
[1 1 2 3]
[1 3 2 0]
#+END_SRC

* Set operations

Get unique values that are sorted.

This requires you expand the dimensions.

#+BEGIN_SRC tf :results output drawer org
import tensorflow as tf

a = tf.constant([3, 1, 2, 1])
b = tf.constant([1, 3, 4, 3])

# This set appears to be sorted, but that is not documented behavior.
s = tf.sets.set_intersection(a[None,:], b[None, :])


with tf.Session() as sess:
    print(sess.run(s).values)
#+END_SRC

#+RESULTS:
#+BEGIN_SRC org
[1 3]
#+END_SRC

* Automatic differentiation in Tensorflow

pydoc:tensorflow.gradients


Tensorflow provides automatic differentiation to get the derivatives of outputs with respect to some inputs. Here is a simple example of computing the derivative of $y = x^2$ at $x=10$. Note that a list is returned from the gradients function.

#+BEGIN_SRC tf :results output drawer org
import tensorflow as tf

x = tf.constant(5.0)
y = x**2

dydx = tf.gradients(y, x)

with tf.Session() as sess:
    print(sess.run(dydx))
#+END_SRC

#+RESULTS:
#+BEGIN_SRC org
[10.0]
#+END_SRC

If the input is a vector, then the derivatives returned will also be a vector.

#+BEGIN_SRC tf :results output drawer org
import tensorflow as tf

x = tf.constant([1.0, 2.0, 5.0])
y = x**2

dydx = tf.gradients(y, x)

with tf.Session() as sess:
    print(sess.run(dydx))
#+END_SRC

#+RESULTS:
#+BEGIN_SRC org
[array([  2.,   4.,  10.], dtype=float32)]
#+END_SRC

You can take the derivative of the output with respect to other nodes in the graph. Here we compute the derivative with the respect to another variable $a$.

#+BEGIN_SRC tf :results output drawer org
import tensorflow as tf

x = tf.constant(5.0)
a = tf.constant(2.0)
y = a * x**2

dyda = tf.gradients(y, a)

with tf.Session() as sess:
    print(sess.run(dyda))
#+END_SRC

#+RESULTS:
#+BEGIN_SRC org
[25.0]
#+END_SRC

There is a subtle point here though. If x is an array, then tf.gradients returns the sum of all the partial derivatives, i.e. out of the box you cannot compute the partial derivative of a as a function of x.

#+BEGIN_SRC tf :results output drawer org
import tensorflow as tf

x = tf.constant([1.0, 2.0, 5.0])
a = tf.constant(2.0)
y = a * x**2

dyda = tf.gradients(y, a)

with tf.Session() as sess:
    print(sess.run(dyda))
    print(sess.run(tf.reduce_sum(x**2)))
#+END_SRC

#+RESULTS:
#+BEGIN_SRC org
[30.0]
30.0
#+END_SRC


To get an element-wise gradient, we have to scan over the elements, and compute the gradient at each point. Here is an example. I have not figured out how to generalize this into a function yet.

#+BEGIN_SRC tf :results output drawer org
import tensorflow as tf

x = tf.constant([1.0, 2.0, 5.0])
a = tf.constant(2.0)

f = a * x**2
dyda = tf.scan(lambda acc, _x: tf.gradients(a * _x**2, a)[0], x)

with tf.Session() as sess:
    print(sess.run(dyda))
    print(sess.run(v))
#+END_SRC

#+RESULTS:
#+BEGIN_SRC org
[  1.   4.  25.]
[30.0, 30.0, 30.0]
#+END_SRC


** inf, nans
   :PROPERTIES:
   :ID:       3909B227-F0AD-456A-959B-578C1FE84777
   :END:

Some times functions don't have derivatives defined at a point, e.g. the square root of zero.

#+BEGIN_SRC tf :results output drawer org
import tensorflow as tf

x = tf.constant([0.0, 0.5, 0.74, 1.0])
y = tf.sqrt(x)

dydx = tf.gradients(y, x)[0]

with tf.Session() as sess:
    print(sess.run(dydx))
#+END_SRC

#+RESULTS:
#+BEGIN_SRC org
[        inf  0.70710683  0.58123815  0.50000006]
#+END_SRC

That infinity can cause a problem for some code, resulting in nans (not a number), etc...

In this case, we have to decide what is appropriate to do, and then design a safe version. Typically this will involve creating a mask of values that are not appropriate, changing those values to something safe, taking the gradient, and then changing the values back to something safe. Here for example, we will mask out the zero values by setting them to 1.0, take the square root, and then set them back to zero afterwards. Then, we won't have a problem with infinities in the derivatives. You have to decide that is the right thing to do.

#+BEGIN_SRC tf :results output drawer org
import tensorflow as tf

x = tf.constant([0.0, 0.5, 0.74, 1.0])   # (ref:test)

mask = tf.equal(x, 0.0) # (ref:mask)
safex = tf.where(mask, tf.ones_like(x), x)
sqx = tf.sqrt(safex)
result = tf.where(mask, tf.zeros_like(x), sqx)

dydx = tf.gradients(result, x)

with tf.Session() as sess:
    print(sess.run(dydx))
#+END_SRC

#+RESULTS:
#+BEGIN_SRC org
[array([ 0.        ,  0.70710683,  0.58123815,  0.50000006], dtype=float32)]
#+END_SRC

In line [[(test)]] we define a constant. In line [[(mask)]] we get the mask.


** Jacobians

https://github.com/tensorflow/tensorflow/issues/675
https://en.wikipedia.org/wiki/Jacobian_matrix_and_determinant

** TODO Hessians

http://runopti.github.io/blog/2016/07/07/HessianComp/

* Applications
** Solving an ODE in tensorflow
   :PROPERTIES:
   :SENT-ON:  Mon Dec 11 07:46:36 2017
   :TO:       kitchin@google.com
   :Message-ID: [[mu4e:msgid:m2y3m9p8rq.fsf@andrew.cmu.edu][Solving an ODE in tensorflow (Mon Dec 11 07:46:36 2017)]]
   :END:

#+BEGIN_SRC tf :results output drawer org
import tensorflow as tf
import numpy as np
import os
os.makedirs('my-model', exist_ok=True)

t = tf.placeholder(tf.float64, (None, 1))

def neural_network(X, layers):
    neurons, activation = layers[0]
    hidden_layers = [tf.layers.dense(X, neurons, activation)]
    for neurons, activation in layers[1:]:
        hidden_layers += [tf.layers.dense(hidden_layers[-1], neurons, activation)]

    return hidden_layers[-1]

def swish(x):
    return x / (1.0 + tf.exp(-x))

time = np.linspace(0, 10).reshape((-1, 1))
Ca = neural_network(t, ((8, swish), (1, tf.identity)))

dCadt = tf.gradients(Ca, t)[0]

k = 0.23
Ca0 = 2.0

deq = dCadt + k * Ca
ic = Ca[0] - Ca0

loss = tf.reduce_mean(tf.square(deq)) + ic**2

train = tf.train.AdamOptimizer(0.001).minimize(loss)

saver = tf.train.Saver()
tf.add_to_collection('train_op', train)
tf.add_to_collection('loss_op', loss)
tf.add_to_collection('ph', t)
tf.add_to_collection('Ca', Ca)

import matplotlib.pyplot as plt

init = tf.global_variables_initializer()
sess = tf.Session()
sess.run(init)

for i in range(5001):
    _, _loss = sess.run([train, loss], feed_dict={t: time})
    if i % 500 == 0:
        print(i, _loss)
        if _loss < 5e-6:
            break

saver.save(sess, './my-model/model', global_step=i)

soln = sess.run(Ca, feed_dict={t: time})
plt.plot(time, soln)
plt.plot(time, Ca0 * np.exp(-k * time), 'r--')
plt.xlim([0, 10])
plt.ylim([0, 2])
plt.savefig('ode.png')
sess.close()
#+END_SRC

#+RESULTS:
#+BEGIN_SRC org
0 [4.22055446]
500 [0.01229707]
1000 [0.00025969]
1500 [4.83326249e-05]
2000 [1.27974797e-05]
2500 [5.63028632e-06]
3000 [3.35930356e-06]
#+END_SRC

[[./ode.png]]

 Restoring the model and continuing. This is moderately awkward.

#+BEGIN_SRC tf :results output drawer org
import tensorflow as tf

import numpy as np
import matplotlib.pyplot as plt
Ca0 = 2.0
k = 0.23

time = np.linspace(0, 10).reshape((-1, 1))

with tf.Session() as sess:
  saver = tf.train.import_meta_graph('my-model/model-5000.meta')
  saver.restore(sess, './my-model/model-5000')
  # tf.get_collection() returns a list. In this example we only want the
  # first one.
  train_op = tf.get_collection('train_op')[0]
  loss = tf.get_collection('loss_op')
  Ca = tf.get_collection('Ca')[0]
  ph = tf.get_collection('ph')[0]
  for step in range(1000):
    _, _loss = sess.run([train_op, loss], feed_dict={ph: time})
    if step % 100 == 0:
      print(_loss)

  plt.plot(time, Ca.eval(feed_dict={ph: time}))
  plt.plot(time, Ca0 * np.exp(-k * time), 'r--')
  saver.save(sess, 'my-model/final', global_step=0)

plt.savefig('restored-result.png')
#+END_SRC

#+RESULTS:
#+BEGIN_SRC org
[array([0.00024064])]
[array([0.00022692])]
[array([0.00021335])]
[array([0.00020004])]
[array([0.00018712])]
[array([0.00017476])]
[array([0.00016312])]
[array([0.00015234])]
[array([0.00014252])]
[array([0.00013374])]
#+END_SRC

[[./restored-result.png]]

** Basic neural network code

In this simple example we create a neural network with two hidden layers. The first one has 10 neurons, activated by a relu function, and the second one has three neurons, with no activation. We feed the network a 5x2 array, and it outputs a 5x3 array. There is no training here, it just illustrates the mechanics of creating and feeding the network.

#+BEGIN_SRC tf :results output drawer org
import tensorflow as tf
import numpy as np

X = tf.placeholder(tf.float64, (None, 2))

def neural_network(X, layers):
    neurons, activation = layers[0]
    hidden_layers = [tf.layers.dense(X, neurons, activation)]
    for neurons, activation in layers[1:]:
        hidden_layers += [tf.layers.dense(hidden_layers[-1], neurons, activation)]
    return hidden_layers[-1]

nn = neural_network(X, ((10, tf.nn.relu), (3, tf.identity)))

init = tf.global_variables_initializer()

with tf.Session() as sess:
    sess.run(init)
    print(sess.run(nn, feed_dict={X: np.random.rand(5, 2)}))
#+END_SRC

#+RESULTS:
#+BEGIN_SRC org
[[ 0.32730352 -0.32113062 -0.57728228]
 [ 0.22432179 -0.23120213 -0.3962985 ]
 [ 0.41064357 -0.19050928 -0.20839852]
 [ 0.36477839 -0.2612205  -0.34333513]
 [ 0.26847371 -0.27772755 -0.45310965]]
#+END_SRC

** Sensitivity analysis

See http://kitchingroup.cheme.cmu.edu/blog/2017/11/15/Sensitivity-analysis-using-automatic-differentiation-in-Python/.

It is a little awkward right now to do this in Tensorflow. It is set up to sum all the gradients we want, so we have to evaluate each one at each point separately. We do this by unstacking the time tensor to get a list of time values, and then get the gradient we want at each time.

#+BEGIN_SRC tf :results output drawer org
import tensorflow as tf
import numpy as np

t = np.linspace(0, 0.5)
time = tf.constant(t)
A0 = 1.0
k1 = tf.constant(3.0, dtype=t.dtype)
k_1 = tf.constant(3.0, dtype=t.dtype)

times = tf.unstack(time)
output = [A0 / (k1 + k_1) * (k1 * tf.exp(-(k1 + k_1) * time) + k_1)
          for time in times]

dAdk1 = [tf.gradients(A, [k1])[0] for A in output]
dAdk_1 = [tf.gradients(A, [k_1])[0] for A in output]

with tf.Session() as sess:
    sk1, sk_1 = sess.run([dAdk1, dAdk_1])

import matplotlib.pyplot as plt
plt.plot(t, np.abs(sk1), t, np.abs(sk_1))
plt.xlim([0, 0.5])
plt.ylim([0, 0.1])
plt.xlabel('t')
plt.legend(['$S_{k1}$', '$S_{k\_1}$'])
plt.savefig('sensitivity-analysis.png')
#+END_SRC

#+RESULTS:
#+BEGIN_SRC org
#+END_SRC

[[./sensitivity-analysis.png]]
* Miscellaneous
 
** tf.squeeze
   
pydoc:tensorflow.squeeze

#+BEGIN_SRC tf :results output drawer org
import tensorflow as tf

a = tf.constant([[[1, 2], [3, 4]]])

with tf.Session() as sess:
    print(sess.run(tf.squeeze(a)))
#+END_SRC

#+RESULTS:
#+BEGIN_SRC org
[[1 2]
 [3 4]]
#+END_SRC


** tf.Print
   

pydoc:tensorflow.Print is hard to understand from the documentation. The idea is that the "input" op is passed through to whatever it is assigned to, and when that op is evaluated, the data will be printed. This is necessary because it forces the op to be part of the graph that is executed.  Furthermore, it prints to stderr of the Tensorflow runtime server, so it is currently not compatible with Jupyter or org-mode.

For completeness, here is a program that uses tf.Print. When you run it at the command line, it will print the the string we put in, the value of a, and the value of 3*a. Note that out of the box the tf.Print output is invisible to Emacs since it ignores stderr.


#+BEGIN_SRC tf :results output drawer org :tangle tfprint.py
import tensorflow as tf
import os
os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'  # ignore irritating log messages

a = tf.constant(2)

a = tf.Print(a, ['something for me', a, 3 * a])

b = a * a

with tf.Session() as sess:
  print(sess.run(b))
#+END_SRC

#+RESULTS:
#+BEGIN_SRC org
4
#+END_SRC

You can modify the Python executable in a header arg like this:

#+BEGIN_EXAMPLE
:python exec 2>&1; python
#+END_EXAMPLE

And then it will capture the stderr.

#+BEGIN_SRC tf :python exec 2>&1; python :results output drawer org :tangle tfprint.py
import tensorflow as tf
import os
os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'  # ignore irritating log messages

a = tf.constant(2)

a = tf.Print(a, ['something for me', a, 3 * a])

b = a * a

with tf.Session() as sess:
  print(sess.run(b))
#+END_SRC

#+RESULTS:
#+BEGIN_SRC org
[something for me][2][6]
4
#+END_SRC

If you don't want to mess with the python command, here is a way to get the output from a shell command into orgmode. Of course, if you run this at the command line, you will see everything in the Terminal.

#+BEGIN_SRC sh :results org
exec 2>&1
python tfprint.py
:
#+END_SRC

#+RESULTS:
#+BEGIN_SRC org
[something for me][2][6]
4
#+END_SRC


That is overall on the irritating side of things to use. You have to define a tensor, then define a tf.Print op that passes the tensor through to get it to evaluate. A lisp would let you write a macro around that. Just saying.

I did find this [[https://gist.github.com/vihari/f9b361058825e16d390f0e443bfdffc7][gist]] that shows a way to get tf.Print to print to stdout using a pyfunc. It looks like this should be ok for differentiability. 

#+BEGIN_SRC tf :results output drawer org
import tensorflow as tf
import sys

def tf_print(op, tensors, message=None):

    def print_message(x):
        sys.stdout.write(message + " %s\n" % x)
        return x

    prints = [tf.py_func(print_message, [tensor], tensor.dtype) 
              for tensor in tensors]
    with tf.control_dependencies(prints):
        op = tf.identity(op)
    return op

x = tf.constant([1, 2, 3])
y = tf.constant([5, 6])
p = x * x
p = tf_print(p, [x, y], "hello")

with tf.Session() as sess:
    p.eval()
    print(sess.run(tf.gradients(p, x)))
#+END_SRC

#+RESULTS:
#+BEGIN_SRC org
hello [5 6]
hello [1 2 3]
[array([2, 4, 6], dtype=int32)]
#+END_SRC



** tf.app
   
Here is a way to use tf.app to make it easy to pass values at the command line.

#+BEGIN_SRC tf :results output drawer org :tangle test.py
import tensorflow as tf

tf.app.flags.DEFINE_boolean("your_flag", False, "A boolean")

FLAGS = tf.app.flags.FLAGS

def main(_):
    print(FLAGS.your_flag)
    a = tf.constant(4)
    a = tf.Print(a, ['Got it'])
    sess = tf.Session()
    print(sess.run(a * a))
    return 

if __name__ == '__main__':
    tf.app.run(main)
#+END_SRC

#+RESULTS:
#+BEGIN_SRC org
False
16
#+END_SRC

#+BEGIN_SRC sh
python test.py
#+END_SRC

#+RESULTS:
| False |
|    16 |

#+BEGIN_SRC sh
python test.py --your_flag=True
#+END_SRC

#+RESULTS:
| True |
|   16 |

The help prints on stderr, so we redirect it here.
#+BEGIN_SRC sh :results output
exec 2>&1
python test.py --help
:
#+END_SRC

#+RESULTS:
: 
:        USAGE: test.py [flags]
: flags:
: 
: test.py:
:   --[no]your_flag: A boolean
:     (default: 'false')
: 
: Try --helpfull to get a list of all flags.

** Saving and restoring
   
Suppose we have this simple TF script. 

#+BEGIN_SRC tf :results output drawer org
import tensorflow as tf

a = tf.Variable(tf.constant(4))
x = tf.placeholder(tf.int32)

y = a * x**2

with tf.Session() as sess:
    sess.run(tf.global_variables_initializer())
    print(sess.run(y, feed_dict={x: 5}))
#+END_SRC

#+RESULTS:
#+BEGIN_SRC org
100
#+END_SRC

It would be nice if we could save it in a way that would allow us to reuse it later. 

*** Saving/restoring with collections

Here is an example of saving some ops from a graph for reuse later.

It seems to be important that you define the saver last to capture all the variables.  

#+BEGIN_SRC tf :results silent
import tensorflow as tf

a = tf.Variable(tf.constant(4))
x = tf.placeholder(tf.int32)

y = a * x**2

tf.add_to_collection('ops', x)
tf.add_to_collection('ops', y)

saver = tf.train.Saver()

with tf.Session() as sess:
    sess.run(tf.global_variables_initializer())   
    saver.save(sess, './square')
#+END_SRC

Now, we can restore the graph and get the relevant ops like this.

#+BEGIN_SRC tf :results output drawer org
import tensorflow as tf
checkpoints_file_name = './square'
saver = tf.train.import_meta_graph(checkpoints_file_name + '.meta')


with tf.Session() as sess:
    saver.restore(sess, checkpoints_file_name)
    x, y = tf.get_collection('ops')
    print(sess.run(y, feed_dict={x: 25}))
#+END_SRC

#+RESULTS:
#+BEGIN_SRC org
2500
#+END_SRC

*** Saving/restoring with named tensors
    
Here is an alternative approach to save/restore using named tensors instead of the collection.

#+BEGIN_SRC tf :results silent
import tensorflow as tf

a = tf.Variable(tf.constant(4), name='a')
x = tf.placeholder(tf.int32, name='x_ph')

# use a tf op to give it a name
y = tf.multiply(a, x**3, name='y')

saver = tf.train.Saver()

with tf.Session() as sess:
    sess.run(tf.global_variables_initializer())   
    saver.save(sess, './cubed')
#+END_SRC

Here we restore the graph, and show all the nodes in it. Then, we recreate the tensors needed to run the ops. We run them, then assign the var a new value, and run it again.

#+BEGIN_SRC tf :results output drawer org
import tensorflow as tf
checkpoints_file_name = './cubed'
saver = tf.train.import_meta_graph(checkpoints_file_name + '.meta')

print([n.name for n in tf.get_default_graph().as_graph_def().node])
print()
with tf.Session() as sess:
    saver.restore(sess, checkpoints_file_name)    
    g = tf.get_default_graph()
    var = g.get_tensor_by_name('a:0')
    x_ph = g.get_tensor_by_name('x_ph:0')
    y = g.get_tensor_by_name('y:0')
    print(sess.run(y, feed_dict={x_ph: 5}))
    print(sess.run(var))
    # give a a new value
    print(sess.run(tf.assign(var, 1)))
    print(sess.run(y, feed_dict={x_ph: 5}))
#+END_SRC

#+RESULTS:
#+BEGIN_SRC org
['Const', 'a', 'a/Assign', 'a/read', 'x_ph', 'pow/y', 'pow', 'y', 'save/Const', 'save/SaveV2/tensor_names', 'save/SaveV2/shape_and_slices', 'save/SaveV2', 'save/control_dependency', 'save/RestoreV2/tensor_names', 'save/RestoreV2/shape_and_slices', 'save/RestoreV2', 'save/Assign', 'save/restore_all', 'init']

500
4
1
125
#+END_SRC

*** Using tf.identity to give operations names

In the example above we had to use a specific syntax like:

=y = tf.multiply(a, x**3, name='y')=

to give the op a name. Here we wrap that in an identity op to achieve the same result, with the more concise syntax.

#+BEGIN_SRC tf :results silent
import tensorflow as tf

a = tf.Variable(tf.constant(4), name='a')
x = tf.placeholder(tf.int32, name='x_ph')


y = tf.identity(a * x**3, name='y')

saver = tf.train.Saver()

with tf.Session() as sess:
    sess.run(tf.global_variables_initializer())   
    saver.save(sess, './cubed_i')
#+END_SRC


#+BEGIN_SRC tf :results output drawer org
import tensorflow as tf
checkpoints_file_name = './cubed_i'
saver = tf.train.import_meta_graph(checkpoints_file_name + '.meta')

with tf.Session() as sess:
    saver.restore(sess, checkpoints_file_name)    
    g = tf.get_default_graph()
    var = g.get_tensor_by_name('a:0')
    x_ph = g.get_tensor_by_name('x_ph:0')
    y = g.get_tensor_by_name('y:0')
    print(sess.run(y, feed_dict={x_ph: 5}))
    print(sess.run(var))
    # give a a new value
    print(sess.run(tf.assign(var, 1)))
    print(sess.run(y, feed_dict={x_ph: 5}))
#+END_SRC

#+RESULTS:
#+BEGIN_SRC org
500
4
1
125
#+END_SRC
** Using Tensorboard
   
[[https://www.tensorflow.org/programmers_guide/summaries_and_tensorboard][Tensorboard]] is a browser-based tool to visualize your Tensorflow programs. You can create summaries of different operations, and view the graph. Usually Tensorboard is used to monitor training, but you can do other things too.

Here is a contrived example where we change a variable and save the changes to Tensorboard.

#+BEGIN_SRC tf :results output drawer org
import tensorflow as tf

v = tf.Variable(0.0, tf.float32)
vsum = tf.summary.scalar("v", v)
summary = tf.summary.merge_all()

with tf.Session() as sess:
    sess.run(tf.global_variables_initializer())
    writer = tf.summary.FileWriter('./summaries', sess.graph)
    
    for i in range(9):
        v = tf.assign(v, float(i**2))
        _v = sess.run(v)
        _summary = sess.run(summary)  # Run this in a separate step
        writer.add_summary(_summary, global_step=i)
        print(i, v.eval())    
#+END_SRC

#+RESULTS:
#+BEGIN_SRC org
0 0.0
1 1.0
2 4.0
3 9.0
4 16.0
5 25.0
6 36.0
7 49.0
8 64.0
#+END_SRC

You won't see anything when you run the block above. You have to launch Tensorboard like this:

#+BEGIN_SRC sh
tensorboard --logdir=./summaries --port=6006 &
google-chrome http://localhost:6006
#+END_SRC

#+RESULTS:
| Created new window in existing browser session. |                           |
| TensorBoard attempted to bind to port 6006      | but it was already in use |

Then there will be a web page open to the summary.
